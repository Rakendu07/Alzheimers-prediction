# -*- coding: utf-8 -*-
"""Alzheimer's Prediction Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LILhcD8w-6okUu8jdheNg7cisSYUjplA
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
import xgboost as xgb # Import XGBoost Classifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import numpy as np
import zipfile # Import zipfile for unzipping
import os # Import os for path operations

# --- Step 1: Load the Tabular Data ---
print("Step 1: Loading the tabular data...")

# Define the path to your uploaded zip file
actual_zip_file_name = "Alzheimer's tabular Dataset.zip" # Assuming this is the actual zip file name uploaded.

# Define the directory where the dataset will be extracted
extract_dir = '/content/extracted_tabular_data/'
os.makedirs(extract_dir, exist_ok=True) # Create the directory if it doesn't exist

try:
    # Check if the zip file exists in the current working directory or /content/
    if os.path.exists(actual_zip_file_name):
        zip_to_extract = actual_zip_file_name
    elif os.path.exists(os.path.join('/content/', actual_zip_file_name)):
        zip_to_extract = os.path.join('/content/', actual_zip_file_name)
    else:
        raise FileNotFoundError(f"Zip file '{actual_zip_file_name}' not found in common locations.")

    print(f"Unzipping '{zip_to_extract}' to '{extract_dir}'...")
    with zipfile.ZipFile(zip_to_extract, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print("Dataset unzipped successfully.")

    # Now, find the actual CSV file inside the extracted directory
    csv_file_name = "alzheimers_disease_data.csv"

    found_csv_path = None
    if os.path.exists(os.path.join(extract_dir, csv_file_name)):
        found_csv_path = os.path.join(extract_dir, csv_file_name)
    else:
        # If not directly there, check for a common nesting level (e.g., a folder named after the zip)
        extracted_contents = os.listdir(extract_dir)
        for item in extracted_contents:
            potential_csv_path = os.path.join(extract_dir, item, csv_file_name)
            if os.path.exists(potential_csv_path):
                found_csv_path = potential_csv_path
                break

    if found_csv_path:
        df = pd.read_csv(found_csv_path)
        print(f"Data loaded successfully from '{found_csv_path}'. First 5 rows:")
        print(df.head())
        print("\nData Info:")
        df.info()
    else:
        raise FileNotFoundError(f"CSV file '{csv_file_name}' not found within '{extract_dir}' after unzipping.")

except FileNotFoundError as fnfe:
    print(f"Error: {fnfe}")
    print("Please ensure the zip file 'Alzheimer\'s tabular Dataset.zip' is uploaded and accessible.")
    exit()
except zipfile.BadZipFile:
    print(f"Error: '{zip_to_extract}' is not a valid zip file or is corrupted.")
    exit()
except Exception as e:
    print(f"An unexpected error occurred during data loading: {e}")
    exit()


# --- Step 2: Preprocessing the Tabular Data ---
print("\nStep 2: Preprocessing the tabular data...")

# Drop irrelevant columns: PatientID and DoctorInCharge
columns_to_drop = []
if 'PatientID' in df.columns:
    columns_to_drop.append('PatientID')
if 'DoctorInCharge' in df.columns:
    columns_to_drop.append('DoctorInCharge')

if columns_to_drop:
    df = df.drop(columns=columns_to_drop)
    print(f"Dropped columns: {columns_to_drop}")
else:
    print("Columns 'PatientID' or 'DoctorInCharge' not found, skipping drop.")


# Identify target variable and features
target_column = 'Diagnosis'
if target_column not in df.columns:
    print(f"Error: Target column '{target_column}' not found in the dataset.")
    exit()

features_df = df.drop(columns=[target_column])
target_series = df[target_column]

# Define class names explicitly based on the dataset's integer encoding
# Assuming 0 -> Demented, 1 -> NonDemented based on common medical context
# If your dataset's 0 and 1 mean something else, adjust these names.
class_names = ['Demented', 'NonDemented']

# Convert the target variable ('Diagnosis') to numerical (0 and 1)
# If 'Diagnosis' is already 0/1 integers, LabelEncoder will just map them to themselves.
label_encoder = LabelEncoder()
target_encoded = label_encoder.fit_transform(target_series)
df[target_column] = target_encoded
print(f"\nTarget variable mapping: {list(label_encoder.classes_)} -> {label_encoder.transform(label_encoder.classes_)}")
# Override label_encoder.classes_ with our defined string names for reporting
label_encoder.classes_ = np.array(class_names)


# Separate features (X) and target (y)
X = df.drop(columns=[target_column])
y = df[target_column]

# Identify numerical and binary/ordinal categorical columns for scaling
numerical_cols = X.columns.tolist()

# Initialize StandardScaler
scaler = StandardScaler()

# Apply scaling to all numerical features
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
print("\nFeatures scaled successfully. First 5 rows of scaled data:")
print(X_scaled_df.head())

# --- Step 3: Data Splitting (Train, Test) ---
print("\nStep 3: Splitting data into training and test sets...")

# We'll use a 80/20 split for train/test. GridSearchCV will handle internal validation.
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled_df, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set shape: {X_train.shape}, {y_train.shape}")
print(f"Test set shape: {X_test.shape}, {y_test.shape}")


# --- Step 4: Model Training (XGBoostClassifier with GridSearchCV) ---
print("\nStep 4: Training XGBoostClassifier model with GridSearchCV...")

# Define the parameter grid for GridSearchCV for XGBoostClassifier
param_grid = {
    'n_estimators': [100, 200, 300], # Number of boosting rounds (trees)
    'learning_rate': [0.01, 0.1, 0.2], # Step size shrinkage to prevent overfitting
    'max_depth': [3, 5, 7], # Maximum depth of a tree
    'subsample': [0.8, 1.0], # Subsample ratio of the training instance
    'colsample_bytree': [0.8, 1.0], # Subsample ratio of columns when constructing each tree
    'gamma': [0, 0.1, 0.2], # Minimum loss reduction required to make a further partition on a leaf node
    'objective': ['binary:logistic'], # For binary classification
    'use_label_encoder': [False], # Suppress warning, usually set to False for newer versions
    'eval_metric': ['logloss'] # Evaluation metric for validation data
}

# Initialize XGBoostClassifier model
base_xgb_model = xgb.XGBClassifier(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=base_xgb_model, param_grid=param_grid,
                           cv=5, scoring='roc_auc', n_jobs=-1, verbose=2)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best model found by GridSearchCV
best_xgb_model = grid_search.best_estimator_

print("\nGridSearchCV completed for XGBoostClassifier.")
print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best ROC AUC score on training data (cross-validation): {grid_search.best_score_:.4f}")
print("Best XGBoostClassifier model trained successfully.")


# --- Step 5: Evaluate using Test Set (using the best model) ---
print("\nStep 5: Evaluating the best model on the Test Set...")
y_test_pred = best_xgb_model.predict(X_test)
y_test_proba = best_xgb_model.predict_proba(X_test)[:, 1] # Probability of the positive class ('NonDemented')

test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred)
test_recall = recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
test_roc_auc = roc_auc_score(y_test, y_test_proba)
test_cm = confusion_matrix(y_test, y_test_pred)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-score: {test_f1:.4f}")
print(f"Test ROC AUC: {test_roc_auc:.4f}")
print("Test Confusion Matrix:\n", test_cm)
# Display detailed classification report for precision, recall, and f1-score per class
print("\nClassification Report (Test Set):\n", classification_report(y_test, y_test_pred, target_names=class_names)) # Use class_names


# --- Step 6: Example of prediction using user input (simulated). ---
print("\nStep 6: Example of prediction using user input (simulated).")

# Create a dummy user input for prediction based on the scaled features
# This should ideally come from actual user input, but for demonstration, we'll use a random sample from the test set.
# The input features must be in the same order and scaled using the same scaler.

# Get a random row from the original DataFrame (before dropping target) to simulate user input
random_idx = np.random.randint(0, len(df))
sample_user_data_original = df.iloc[random_idx].drop(columns=[target_column])

# Ensure the order of columns matches X used for training
sample_user_data_processed = sample_user_data_original[X.columns].values.reshape(1, -1)

# Scale the user input using the trained scaler
sample_user_data_scaled = scaler.transform(sample_user_data_processed)

# Make a prediction using the best_xgb_model
user_prediction = best_xgb_model.predict(sample_user_data_scaled)
user_prediction_proba = best_xgb_model.predict_proba(sample_user_data_scaled)[:, 1]

# Decode the prediction back to original labels
predicted_diagnosis = label_encoder.inverse_transform(user_prediction)[0]
true_diagnosis = label_encoder.inverse_transform([y.iloc[random_idx]])[0]

print(f"\nSimulated User Input Data (original values for context):\n{sample_user_data_original}")
print(f"\nPredicted Diagnosis: {predicted_diagnosis}")
print(f"Probability of Demented: {user_prediction_proba[0]:.4f}")
print(f"Actual Diagnosis (for this simulated input): {true_diagnosis}")

# --- Evaluation Metrics Summary ---
print("\n--- Summary of Evaluation Metrics (Test Set) ---")
print(f"Accuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-score: {test_f1:.4f}")
print(f"ROC AUC: {test_roc_auc:.4f}")
print("Confusion Matrix:\n", test_cm)

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16 # For transfer learning
from tensorflow.keras.optimizers import Adam
import os
import zipfile
import numpy as np # Added for general utility, though not strictly used in this snippet

print("\n--- MRI Image Prediction using CNN (Standalone) ---")

# Define the path to your uploaded zip file containing MRI images
zip_file_path = '/content/archive (2).zip'
# Define the base directory where the dataset will be extracted.
extract_base_dir = '/content/'

# Check if the zip file exists
if not os.path.exists(zip_file_path):
    print(f"Error: Zip file not found at '{zip_file_path}'.")
    print("Please ensure the MRI image zip file is uploaded to this path.")
else:
    # Unzip the dataset
    print(f"Unzipping '{zip_file_path}' to '{extract_base_dir}'...")
    try:
        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
            zip_ref.extractall(extract_base_dir)
        print("Dataset unzipped successfully.")
    except Exception as e:
        print(f"Error unzipping file: {e}")
        print("Please ensure the zip file is valid and not corrupted.")
        exit()

    # --- Robustly find the actual dataset root directory ---
    actual_dataset_root = None
    for root, dirs, files in os.walk(extract_base_dir):
        if 'train' in dirs and 'test' in dirs and \
           os.path.isdir(os.path.join(root, 'train')) and \
           os.path.isdir(os.path.join(root, 'test')):
            actual_dataset_root = root
            break

    if actual_dataset_root:
        print(f"Automatically detected actual dataset root directory: '{actual_dataset_root}'")
        final_data_dir = actual_dataset_root
    else:
        print("\nCould not automatically determine the dataset's root directory containing 'train' and 'test'.")
        print(f"Contents of '{extract_base_dir}' after unzipping (first level):")
        for item in os.listdir(extract_base_dir):
            print(f"- {item}")
        print("\nPlease manually inspect the extracted contents and update 'final_data_dir' in the code.")
        exit()


    # Define image dimensions and batch size
    IMG_HEIGHT = 128
    IMG_WIDTH = 128
    BATCH_SIZE = 32

    # Define the path to your MRI dataset's train and test directories after extraction
    train_dir = os.path.join(final_data_dir, 'train')
    test_dir = os.path.join(final_data_dir, 'test')

    print(f"Attempting to use train_dir: {train_dir}")
    print(f"Attempting to use test_dir: {test_dir}")

    # Verify extracted directories exist
    if not os.path.exists(train_dir) or not os.path.exists(test_dir):
        print(f"Final Error: Extracted MRI dataset directories not found at '{train_dir}' or '{test_dir}'.")
        print("This should not happen if auto-detection worked. Please verify the zip file content.")
    else:
        # Use ImageDataGenerator for data augmentation and loading
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            horizontal_flip=True,
            validation_split=0.2
        )

        test_datagen = ImageDataGenerator(rescale=1./255)

        train_generator = train_datagen.flow_from_directory(
            train_dir,
            target_size=(IMG_HEIGHT, IMG_WIDTH),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            subset='training',
            seed=42
        )

        validation_generator = train_datagen.flow_from_directory(
            train_dir,
            target_size=(IMG_HEIGHT, IMG_WIDTH),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            subset='validation',
            seed=42
        )

        test_generator = test_datagen.flow_from_directory(
            test_dir,
            target_size=(IMG_HEIGHT, IMG_WIDTH),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            shuffle=False,
            seed=42
        )

        num_classes = len(train_generator.class_indices)

        # --- Transfer Learning: Load VGG16 base model ---
        # Include_top=False means we don't include the ImageNet classification layers
        # weights='imagenet' means use pre-trained weights
        # input_shape matches our image dimensions
        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))

        # Freeze the convolutional base
        base_model.trainable = False

        # Create a new model on top of the pre-trained base
        inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))
        x = base_model(inputs, training=False) # Important: set training=False when using base_model as feature extractor
        x = Flatten()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(num_classes, activation='softmax')(x)

        mri_model = Model(inputs, outputs)

        mri_model.compile(optimizer=Adam(learning_rate=0.0001), # Use a small learning rate for transfer learning
                          loss='categorical_crossentropy',
                          metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])

        print("\nTraining CNN model (with VGG16 Transfer Learning) for MRI images...")
        history = mri_model.fit(
            train_generator,
            epochs=10, # Initial training of the new top layers
            validation_data=validation_generator
        )

        # --- Optional: Fine-tuning the top layers of the base model ---
        # Unfreeze some layers of the base model for fine-tuning
        # This allows the model to adapt the pre-trained features to your specific task
        # base_model.trainable = True
        # # Fine-tune from this layer onwards
        # for layer in base_model.layers[:-4]: # Unfreeze the last 4 convolutional layers
        #     layer.trainable = False
        #
        # mri_model.compile(optimizer=Adam(learning_rate=0.00001), # Even smaller learning rate for fine-tuning
        #                   loss='categorical_crossentropy',
        #                   metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])
        #
        # print("\nFine-tuning CNN model (VGG16) for MRI images...")
        # history_fine_tune = mri_model.fit(
        #     train_generator,
        #     epochs=10, # Additional epochs for fine-tuning
        #     validation_data=validation_generator,
        #     initial_epoch=history.epoch[-1] # Start from where previous training left off
        # )


        print("\nEvaluating CNN model on MRI test set...")
        mri_eval_results = mri_model.evaluate(test_generator)
        print(f"CNN Test Loss: {mri_eval_results[0]:.4f}")
        print(f"CNN Test Accuracy: {mri_eval_results[1]:.4f}")
        print(f"CNN Test Precision: {mri_eval_results[2]:.4f}")
        print(f"CNN Test Recall: {mri_eval_results[3]:.4f}")
        print(f"CNN Test ROC AUC: {mri_eval_results[4]:.4f}")

        y_test_proba_mri = mri_model.predict(test_generator)
        print("\nCNN predictions (probabilities) generated for combination.")

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from PIL import Image
import io
import os

print("--- Simulated Combined Prediction in Colab (All Clinical Features) ---")

# --- Mocking Scaler and LabelEncoder (for demonstration) ---
# In a real scenario, you would load your trained scaler and label_encoder.
# Example:
# import joblib
# scaler = joblib.load('path/to/your/scaler.pkl')
# label_encoder = joblib.load('path/to/your/label_encoder.pkl')

class MockScaler:
    def __init__(self, feature_names):
        self.feature_names = feature_names
        # In a real scaler, you'd have self.mean_ and self.scale_
        # For this mock, we'll just apply a simple scaling rule.

    def transform(self, data_dict):
        # This mock assumes data_dict contains all feature names from self.feature_names
        scaled_data = {}
        for feature in self.feature_names:
            value = data_dict.get(feature, 0.0) # Get value, default to 0 if missing
            if feature in ['Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity',
                           'DietQuality', 'SleepQuality', 'SystolicBP', 'DiastolicBP',
                           'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL',
                           'CholesterolTriglycerides', 'MMSE', 'FunctionalAssessment', 'ADL']:
                scaled_data[feature] = float(value) / 100.0 # Simple scaling for continuous values
            else: # Binary or ordinal categorical features
                scaled_data[feature] = float(value) # Keep as is (0 or 1)
        return scaled_data # Return as a dictionary of scaled values

class MockLabelEncoder:
    def inverse_transform(self, prediction_array):
        return np.array(['NonDemented' if p == 1 else 'Demented' for p in prediction_array])

# Define all feature names as per your NEW dataset (excluding PatientID, DoctorInCharge, Diagnosis)
all_feature_names = [
    'Age', 'Gender', 'Ethnicity', 'EducationLevel', 'BMI', 'Smoking', 'AlcoholConsumption',
    'PhysicalActivity', 'DietQuality', 'SleepQuality', 'FamilyHistoryAlzheimers',
    'CardiovascularDisease', 'Diabetes', 'Depression', 'HeadInjury', 'Hypertension',
    'SystolicBP', 'DiastolicBP', 'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL',
    'CholesterolTriglycerides', 'MMSE', 'FunctionalAssessment', 'MemoryComplaints',
    'BehavioralProblems', 'ADL', 'Confusion', 'Disorientation', 'PersonalityChanges',
    'DifficultyCompletingTasks', 'Forgetfulness'
]

mock_scaler = MockScaler(all_feature_names)
mock_label_encoder = MockLabelEncoder()

# --- Mocking Model Prediction Functions ---
def simulate_tabular_prediction(scaled_data_dict):
    """
    Simulates a probability of 'Demented' based on scaled tabular data.
    Input: scaled_data_dict (dictionary of scaled features)
    Output: probability of Demented (float between 0 and 1)
    """
    # This is a more complex heuristic using more features, but still a simulation.
    # In a real scenario, this would be your best_svm_model.predict_proba(scaled_data_array)

    prob_demented = 0.5 # Base probability

    # Factors increasing Demented probability (simplified logic)
    if scaled_data_dict.get('MMSE', 1.0) < 0.25: prob_demented += 0.15 # Lower MMSE
    if scaled_data_dict.get('Age', 0.5) > 0.7: prob_demented += 0.1 # Higher Age
    if scaled_data_dict.get('FamilyHistoryAlzheimers', 0) == 1: prob_demented += 0.1 # Family History
    if scaled_data_dict.get('MemoryComplaints', 0) == 1: prob_demented += 0.08 # Memory Complaints
    if scaled_data_dict.get('Confusion', 0) == 1: prob_demented += 0.07 # Confusion
    if scaled_data_dict.get('ADL', 1.0) < 0.5: prob_demented += 0.05 # Lower ADL score
    if scaled_data_dict.get('Ethnicity', 0) == 3: prob_demented += 0.04 # Example: specific ethnicity might increase risk

    # Factors decreasing Demented probability (simplified logic)
    if scaled_data_dict.get('DietQuality', 0.5) > 0.8: prob_demented -= 0.03 # Good Diet
    if scaled_data_dict.get('PhysicalActivity', 0.5) > 0.7: prob_demented -= 0.02 # Active

    return np.clip(prob_demented, 0.0, 1.0) # Ensure probability is within [0, 1]

def simulate_mri_prediction(image_data_bytes=None):
    """
    Simulates a probability of 'Demented' based on whether an MRI image is provided.
    Input: image_data_bytes (bytes) - Raw image data, or None if no image.
    Output: probability of Demented (float between 0 and 1)
    """
    if image_data_bytes is not None:
        # In a real scenario, you'd preprocess image_data_bytes and feed to your CNN.
        # For simulation, just the presence of data indicates an image was provided.

        # Simulate multi-class output from CNN, then convert to binary 'Demented' probability.
        # CNN Class Indices: {'Mild Impairment': 0, 'Moderate Impairment': 1, 'No Impairment': 2, 'Very Mild Impairment': 3}
        # Assuming 'No Impairment' (index 2) maps to NonDemented, others to Demented.
        mock_probs_4_classes = np.array([0.2, 0.1, 0.4, 0.3]) # Example distribution: 40% No Impairment

        no_impairment_prob = mock_probs_4_classes[2] # Probability for 'No Impairment'
        demented_prob = 1.0 - no_impairment_prob # Sum of Mild, Moderate, VeryMild Impairment
        return demented_prob
    return 0.5 # Default if no image is considered

# --- User Input for Prediction ---
print("\nPlease enter clinical data for the patient:")
user_tabular_data_raw = {}
try:
    for feature in all_feature_names:
        if feature in ['Age', 'EducationLevel', 'SystolicBP', 'DiastolicBP']:
            user_input = input(f"Enter {feature} (integer): ")
            user_tabular_data_raw[feature] = int(user_input)
        elif feature in ['Gender', 'Smoking', 'FamilyHistoryAlzheimers', 'CardiovascularDisease',
                         'Diabetes', 'Depression', 'HeadInjury', 'Hypertension', 'Ethnicity', # Added Ethnicity here
                         'MemoryComplaints', 'BehavioralProblems', 'Confusion',
                         'Disorientation', 'PersonalityChanges', 'DifficultyCompletingTasks', 'Forgetfulness']:
            user_input = input(f"Enter {feature} (0 or 1): ")
            user_tabular_data_raw[feature] = int(user_input)
        else: # float type
            user_input = input(f"Enter {feature} (e.g., 25.5): ")
            user_tabular_data_raw[feature] = float(user_input)

    # --- MRI Image Input by Path in Colab ---
    print("\n--- MRI Image Input ---")
    print("Please provide the path to your MRI image (e.g., /content/VeryMildImpairment (996).jpg).")
    print("Leave blank and press Enter if you don't want to provide an image.")
    image_path = input("Image path: ").strip()

    mri_image_bytes = None
    if image_path:
        if os.path.exists(image_path):
            try:
                with open(image_path, 'rb') as f:
                    mri_image_bytes = f.read()
                print(f"Image loaded from path: {image_path} ({len(mri_image_bytes)} bytes)")

                # Optional: Verify image can be opened (requires Pillow: pip install Pillow)
                try:
                    Image.open(io.BytesIO(mri_image_bytes))
                    print("Image successfully opened (conceptually).")
                except Exception as e:
                    print(f"Warning: Could not open image from path. Ensure it's a valid image format. Error: {e}")
                    mri_image_bytes = None # Treat as no image if it can't be opened
            except Exception as e:
                print(f"Error reading image from path '{image_path}': {e}")
                mri_image_bytes = None
        else:
            print(f"Error: Image file not found at '{image_path}'. No MRI image will be used for prediction.")
    else:
        print("No MRI image path provided.")

except ValueError as ve:
    print(f"Invalid input. Please ensure you enter the correct data type for each field. Error: {ve}")
    exit()
except Exception as e:
    print(f"An unexpected error occurred during input: {e}")
    exit()


# --- Process Tabular Data ---
print("\n--- Processing Tabular Data ---")
# Display raw input for context
print("Simulated User Input Data (raw values):")
print(pd.Series(user_tabular_data_raw))

# Scale all tabular data using the mock scaler
scaled_tabular_data = mock_scaler.transform(user_tabular_data_raw)
print("\nSimulated Scaled Tabular Data (first 5 features):")
# Convert to Series for easy display of first few items
print(pd.Series(scaled_tabular_data).head())


tabular_prob_demented = simulate_tabular_prediction(scaled_tabular_data)
print(f"\nSimulated Tabular Model Probability of Demented: {tabular_prob_demented:.4f}")

# --- Process MRI Image Data ---
print("\n--- Processing MRI Image Data ---")
mri_prob_demented = simulate_mri_prediction(mri_image_bytes)
print(f"Simulated MRI Model Probability of Demented: {mri_prob_demented:.4f}")

# --- Combine Predictions (Late Fusion - Weighted Average) ---
print("\n--- Combining Predictions ---")
weight_tabular = 0.5
weight_mri = 0.5
combined_probability = (weight_tabular * tabular_prob_demented) + (weight_mri * mri_prob_demented)
print(f"Combined Probability of Demented: {combined_probability:.4f}")

# Convert combined probability to binary prediction
# Recall: 0 for Demented, 1 for NonDemented from your LabelEncoder mapping
final_prediction_binary = 0 if combined_probability > 0.5 else 1
decoded_prediction = mock_label_encoder.inverse_transform(np.array([final_prediction_binary]))[0]

print(f"\nFinal Combined Predicted Diagnosis: {decoded_prediction}")
print("\n--- End of Prediction ---")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, RocCurveDisplay
from sklearn.preprocessing import LabelEncoder # Needed if you want to use class names
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report # Added for other metrics

print("--- Evaluation Metrics Generation ---")

# --- 1. Load Test Data and Predictions from Model Training ---
# In a real scenario, these variables (y_test, y_test_pred, y_test_proba)
# would be available in your Colab environment after running the
# 'Alzheimer's Prediction Model - Tabular Data (Improved XGBoost)' Canvas.

# For demonstration purposes, if you run this script alone, these variables
# would need to be manually assigned or loaded from saved model outputs.
# Example assignments (replace with actual variables from your model training):
# y_true = y_test # From your model training script
# y_pred = y_test_pred # From your model training script
# y_proba = y_test_proba # From your model training script

# Placeholder values for demonstration if actual variables are not available
# These are just to make the script runnable in isolation for testing the plotting.
try:
    # Attempt to use variables from the main model training script if they exist
    y_true = y_test
    y_pred = y_test_pred
    y_proba = y_test_proba
    print("Using actual test data and predictions from the model training script.")
except NameError:
    # If variables are not defined (e.g., running this cell alone)
    print("Warning: Actual test data and predictions not found. Using example data for demonstration.")
    y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
                       0, 1, 0, 1, 0, 1, 0, 1, 0, 1]) # Example: 20 samples
    y_pred = np.array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
                       0, 1, 0, 1, 0, 0, 1, 1, 0, 1]) # Example predictions
    y_proba = np.array([0.4, 0.6, 0.7, 0.8, 0.3, 0.9, 0.2, 0.1, 0.55, 0.65,
                        0.35, 0.75, 0.15, 0.85, 0.45, 0.25, 0.6, 0.7, 0.3, 0.95])


# Define class labels for better readability of the confusion matrix and classification report
# These correspond to the integer encoding (0 and 1) in your dataset.
class_names = ['Demented', 'NonDemented']

print(f"True Labels (y_true) shape: {y_true.shape}")
print(f"Predicted Labels (y_pred) shape: {y_pred.shape}")
print(f"Predicted Probabilities (y_proba) shape: {y_proba.shape}")

# --- 2. Generate and Display Confusion Matrix ---
print("\n--- Confusion Matrix ---")
# Ensure labels are explicitly passed to handle cases where not all classes might be in y_true/y_pred
cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
print("Raw Confusion Matrix:\n", cm)

# Display the confusion matrix with labels
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
fig, ax = plt.subplots(figsize=(6, 6))
disp.plot(cmap=plt.cm.Blues, ax=ax)
ax.set_title('Confusion Matrix')
plt.show()

# --- 3. Generate and Plot ROC Curve ---
print("\n--- ROC Curve ---")

# Calculate ROC curve and AUC (Area Under the Curve)
fpr, tpr, thresholds = roc_curve(y_true, y_proba)
roc_auc = auc(fpr, tpr)

print(f"False Positive Rates (FPR): {fpr}")
print(f"True Positive Rates (TPR): {tpr}")
print(f"Thresholds: {thresholds}")
print(f"ROC AUC: {roc_auc:.4f}")

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# --- 4. Display Other Common Metrics ---
print("\n--- Other Common Metrics ---")
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print("\nClassification Report:\n", classification_report(y_true, y_pred, target_names=class_names))